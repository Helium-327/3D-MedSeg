{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualized Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可视化流程**\n",
    "\n",
    "- 加载模型\n",
    "- 加载数据\n",
    "- 使用模型推理\n",
    "- 可视化推理结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing from timm.models.layers is deprecated, please import via timm.layers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from itertools import product\n",
    "import shutil\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import RMSprop, AdamW\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.BraTS21 import BraTS21_3D\n",
    "from datasets.transforms import Compose, FrontGroundNormalize, RandomCrop3D, ToTensor\n",
    "from lossFunc import DiceLoss, CELoss\n",
    "from metrics import *\n",
    "from utils.logger_tools import custom_logger, get_current_date, get_current_time\n",
    "from utils.ckpt_tools import load_checkpoint\n",
    "from nnArchitecture.baselines.UNet3d import UNet3D\n",
    "from nnArchitecture.baselines.AttentionUNet import AttentionUNet3D\n",
    "\n",
    "from nnArchitecture.optimization_nets.DasppResAtteUNet import DasppResAtteUNet\n",
    "from nnArchitecture.optimization_nets.ScgaResAtteUNet import ScgaResAtteUNet\n",
    "from nnArchitecture.optimization_nets.AA_UNet import AAUNet\n",
    "\n",
    "from nnArchitecture.ref_homo_nets.unetr import UNETR\n",
    "from nnArchitecture.ref_homo_nets.unetrpp import UNETR_PP\n",
    "from nnArchitecture.ref_homo_nets.segFormer3d import SegFormer3D\n",
    "\n",
    "from nnArchitecture.ref_hetero_nets.Mamba3d import Mamba3d\n",
    "from nnArchitecture.ref_hetero_nets.MogaNet import MogaNet\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"加载模型\"\"\"\n",
    "    if model_name == 'UNet3D':\n",
    "        model = UNet3D(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'AttentionUNet3D':\n",
    "        model = AttentionUNet3D(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'unetr':\n",
    "        model = UNETR(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'UNETR':\n",
    "        model = UNETR_PP(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'SegFormer3D':\n",
    "        model = SegFormer3D(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'Mamba3d':\n",
    "        model = Mamba3d(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'MogaNet':\n",
    "        model = MogaNet(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'DasppResAtteUNet':\n",
    "        model = DasppResAtteUNet(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'ScgaResAtteUNet':\n",
    "        model = ScgaResAtteUNet(in_channels=4, out_channels=4)\n",
    "    elif model_name == 'AAUNet':\n",
    "        model = AAUNet(in_channels=4, out_channels=4)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 获取nii文件地址\n",
    "def get_nii_path(out_dir, model_name, patient_idx, modal):\n",
    "    return f'{out_dir}/{model_name}/outputs/BraTS2021_{patient_idx}/BraTS2021_{patient_idx}_{modal}.nii.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_csv, local_train=True, test_length=10, batch_size=1, num_workers=4):\n",
    "    \"\"\"加载数据集\"\"\"\n",
    "    TransMethods_test = Compose([\n",
    "        ToTensor(),\n",
    "        RandomCrop3D(size=(155, 240, 240)),\n",
    "        FrontGroundNormalize(),\n",
    "    ])\n",
    "\n",
    "    test_dataset = BraTS21_3D(\n",
    "        data_file=test_csv,\n",
    "        transform=TransMethods_test,\n",
    "        local_train=local_train,\n",
    "        length=test_length,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True  # 减少 worker 初始化时间\n",
    "    )\n",
    "    \n",
    "    print(f\"已加载测试数据: {len(test_loader)}\")\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 滑窗推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载测试数据: 10\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "factor too big, channels // self.group > 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m attention_unet_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttentionUNet3D\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m attention_unet_daspp_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDasppResAtteUNet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m attention_unet_scga_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScgaResAtteUNet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m attention_unet_aa_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAUNet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#TODO: 修改（添加）模型路径\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/BraTS_Solution/src/optimized_inference.py:73\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m     model \u001b[38;5;241m=\u001b[39m DasppResAtteUNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScgaResAtteUNet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mScgaResAtteUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAUNet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     75\u001b[0m     model \u001b[38;5;241m=\u001b[39m AAUNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/BraTS_Solution/src/nnArchitecture/optimization_nets/ScgaResAtteUNet.py:328\u001b[0m, in \u001b[0;36mScgaResAtteUNet.__init__\u001b[0;34m(self, in_channels, out_channels, f_list, trilinear)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMaxPool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool3d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv1 \u001b[38;5;241m=\u001b[39m ResConv3D(in_channels, f_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv2 \u001b[38;5;241m=\u001b[39m \u001b[43mResConv3DwithSCGA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv3 \u001b[38;5;241m=\u001b[39m ResConv3DwithSCGA(f_list[\u001b[38;5;241m1\u001b[39m], f_list[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv4 \u001b[38;5;241m=\u001b[39m ResConv3DwithSCGA(f_list[\u001b[38;5;241m2\u001b[39m], f_list[\u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[0;32m~/workspace/BraTS_Solution/src/nnArchitecture/optimization_nets/ScgaResAtteUNet.py:215\u001b[0m, in \u001b[0;36mResConv3DwithSCGA.__init__\u001b[0;34m(self, in_channels, out_channels)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_channels, out_channels):\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mSCGA\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    216\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv3d(in_channels, out_channels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    217\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm3d(out_channels),\n\u001b[1;32m    218\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    219\u001b[0m         nn\u001b[38;5;241m.\u001b[39mConv3d(out_channels, out_channels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    220\u001b[0m         nn\u001b[38;5;241m.\u001b[39mBatchNorm3d(out_channels),\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv3d(in_channels, out_channels, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m in_channels \u001b[38;5;241m!=\u001b[39m out_channels \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/workspace/BraTS_Solution/src/nnArchitecture/optimization_nets/improve_components/scga.py:79\u001b[0m, in \u001b[0;36mSCGA.__init__\u001b[0;34m(self, channels, factor)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m(SCGA, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m factor\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactor too big, channels // self.group > 4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maveragePooling \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool3d((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# 3D 全局平均池化\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: factor too big, channels // self.group > 4"
     ]
    }
   ],
   "source": [
    "from optimized_inference import inference, load_model, load_data\n",
    "\n",
    "\n",
    "csv_file = '/root/workspace/VoxelMedix/data/raw/brats21_original/test.csv'\n",
    "out_dir = '/mnt/d/results'\n",
    "\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    out_dir = '../../output'\n",
    "    \n",
    "model_names = ['UNet3D', 'AttentionUNet3D']\n",
    "test_df = pd.read_csv(csv_file)\n",
    "test_loader = load_data(csv_file)\n",
    "\n",
    "unet_model = load_model('UNet3D') \n",
    "attention_unet_model = load_model('AttentionUNet3D')\n",
    "attention_unet_daspp_model = load_model('DasppResAtteUNet')\n",
    "attention_unet_scga_model = load_model('ScgaResAtteUNet')\n",
    "attention_unet_aa_model = load_model('AAUNet')\n",
    "\n",
    "#TODO: 修改（添加）模型路径\n",
    "ckpt_path = {\n",
    "    'UNet3D': f'{out_dir}/1_UNet3D/checkpoints/best@e117_UNet3D__diceloss0.1605_dice0.8397_2025-02-16_18-07-54_19.pth',\n",
    "    'AttentionUNet3D': f'{out_dir}/2_AttentionUNet3D/checkpoints/best@e187_AttentionUNet3D__diceloss0.1476_dice0.8526_2025-02-15_22-19-03_21.pth',\n",
    "    'AttnUNet3D_DenseASPP': f'{out_dir}/2_AttentionUNet3D_DenseASPP/checkpoints/best@e50_DasppResAtteUNet__diceloss0.1436_dice0.8567_2025-02-25_18-24-52_11.pth',\n",
    "    'AttnUNet3D_SCGA': f'{out_dir}/2_AttentionUNet3D_SCGA/checkpoints/best@e53_ScgaResAtteUNet__diceloss0.1556_dice0.8447_2025-02-25_18-29-04_14.pth',\n",
    "    \n",
    "    'AttnUNet3D_AA': f'{out_dir}/3_AA_UNet/checkpoints/best@e136_AAUNet__diceloss0.1404_dice0.8599_2025-02-25_19-55-50_9.pth'\n",
    "}\n",
    "\n",
    "\n",
    "def cfg_generator(model, \n",
    "                  ckpt_path=None,\n",
    "                  test_df=test_df, \n",
    "                  test_loader=test_loader, \n",
    "                  out_dir=out_dir, \n",
    "                  scaler=GradScaler(), \n",
    "                  metricer=EvaluationMetrics()):\n",
    "    return {\n",
    "        'test_df': test_df,\n",
    "        'test_loader': test_loader,\n",
    "        'output_root': out_dir,\n",
    "        'model': model,\n",
    "        'metricer': metricer,\n",
    "        'scaler': scaler,\n",
    "        'optimizer': AdamW(model.parameters(), lr=0.0001, betas=(0.9, 0.99), weight_decay=0.00001),\n",
    "        'ckpt_path': ckpt_path,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化配置\n",
    "# unet_cfg = cfg_generator(unet_model, ckpt_path=ckpt_path['UNet3D'])\n",
    "# attention_unet_cfg = cfg_generator(attention_unet_model, ckpt_path=ckpt_path['AttentionUNet3D'])\n",
    "# attention_unet_denseaspp_cfg = cfg_generator(attention_unet_daspp_model, ckpt_path=ckpt_path['AttnUNet3D_DenseASPP'])\n",
    "# attention_unet_scga_cfg = cfg_generator(attention_unet_scga_model, ckpt_path=ckpt_path['AttnUNet3D_SCGA'])\n",
    "# attention_unet_aa_cfg = cfg_generator(attention_unet_aa_model, ckpt_path=ckpt_path['AttnUNet3D_AA'])\n",
    "\n",
    "# 执行推理\n",
    "# # unet_results = inference(**unet_cfg)\n",
    "# attention_unet_results = inference(**attention_unet_cfg)\n",
    "# attention_unet_denseaspp_results = inference(**attention_unet_denseaspp_cfg)\n",
    "# attention_unet_scga_results = inference(**attention_unet_scga_cfg)\n",
    "# attention_unet_aa_results = inference(**attention_unet_aa_cfg)\n",
    "\n",
    "\n",
    "# attention_unet_config = {\n",
    "#     'test_df': test_df,\n",
    "#     'test_loader': test_loader,\n",
    "#     'output_root': out_dir,\n",
    "#     'model': attention_unet_model,\n",
    "#     'metricer': EvaluationMetrics(),\n",
    "#     'scaler': GradScaler(),\n",
    "#     'optimizer': AdamW(attention_unet_model.parameters(), lr=0.0001, betas=(0.9, 0.99), weight_decay=0.00001),\n",
    "#     'ckpt_path': f'{out_dir}/2_AttentionUNet3D/checkpoints/best@e187_AttentionUNet3D__diceloss0.1476_dice0.8526_2025-02-15_22-19-03_21.pth'\n",
    "# }\n",
    "# attention_unet_denseaspp_config = {\n",
    "#     'test_df': test_df,\n",
    "#     'test_loader': test_loader,\n",
    "#     'output_root': out_dir,\n",
    "#     'model': attention_unet_denseaspp_model,\n",
    "#     'metricer': EvaluationMetrics(),\n",
    "#     'scaler': GradScaler(),\n",
    "#     'optimizer': AdamW(attention_unet_denseaspp_model.parameters(), lr=0.0001, betas=(0.9, 0.99), weight_decay=0.00001),\n",
    "#     'ckpt_path': f'{out_dir}/2_AttentionUNet3D_DenseASPP/checkpoints/best@e50_DasppResAtteUNet__diceloss0.1436_dice0.8567_2025-02-25_18-24-52_11.pth'\n",
    "# }\n",
    "# attention_unet_scga_config = {\n",
    "#     'test_df': test_df,\n",
    "#     'test_loader': test_loader,\n",
    "#     'output_root': out_dir,\n",
    "#     'model': attention_unet_scga_model,\n",
    "#     'metricer': EvaluationMetrics(),\n",
    "#     'scaler': GradScaler(),\n",
    "#     'optimizer': AdamW(attention_unet_scga_model.parameters(), lr=0.0001, betas=(0.9, 0.99), weight_decay=0.00001),\n",
    "#     'ckpt_path': f'{out_dir}/2_AttentionUNet3D_SCGA/checkpoints/best@e53_ScgaResAtteUNet__diceloss0.1556_dice0.8447_2025-02-25_18-29-04_14.pth'\n",
    "# }\n",
    "\n",
    "# attention_aa_unet_cfg = {\n",
    "#     'test_df': test_df,\n",
    "#     'test_loader': test_loader,\n",
    "#     'output_root': out_dir,\n",
    "#     'model': attention_unet_aa_model,\n",
    "#     'metricer': EvaluationMetrics(),\n",
    "#     'scaler': GradScaler(),\n",
    "#     'optimizer': AdamW(attention_unet_aa_model.parameters(), lr=0.0001, betas=(0.9, 0.99), weight_decay=0.00001),\n",
    "#     'ckpt_path': f'{out_dir}/3_AA_UNet/checkpoints/best@e136_AAUNet__diceloss0.1404_dice0.8599_2025-02-25_19-55-50_9.pth'\n",
    "#     }\n",
    "\n",
    "# 执行推理\n",
    "# unet_results = inference(**unet_config)\n",
    "# attention_unet_results = inference(**attention_unet_config)\n",
    "# attention_unet_denseaspp_results = inference(**attention_unet_denseaspp_config)\n",
    "# attention_unet_scga_results = inference(**attention_unet_scga_config)\n",
    "# attention_unet_aa_results = inference(**attention_aa_unet_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/root/workspace/BraTS_Solution/results/best@e53_ScgaResAtteUNet__diceloss0.1556_dice0.8447_2025-02-25_18-29-04_14.pth'\n",
    "\n",
    "checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "saved_keys = checkpoint['model_state_dict'].keys()\n",
    "current_model = ScgaResAtteUNet()\n",
    "current_keys = current_model.state_dict().keys()\n",
    "\n",
    "missing = [k for k in saved_keys if k not in current_keys]\n",
    "unexpected = [k for k in current_keys if k not in saved_keys]\n",
    "\n",
    "print(\"Missing in current model:\", missing)\n",
    "print(\"Unexpected in current model:\", unexpected)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "patient_idx = '00150'\n",
    "output_dir = '/mnt/d/results'\n",
    "\n",
    "nii_path = f'{output_dir}/1_UNet3D/outputs/BraTS2021_{patient_idx}/BraTS2021_{patient_idx}_flair.nii.gz'\n",
    "\n",
    "gt_path = f'{output_dir}/1_UNet3D/outputs/BraTS2021_{patient_idx}/BraTS2021_{patient_idx}_seg.nii.gz'\n",
    "\n",
    "\n",
    "pred_paths = {\n",
    "    'UNet3D': get_nii_path(output_dir, '1_UNet3D', patient_idx, 'pred'),\n",
    "    'AttentionUNet3D': get_nii_path(output_dir, '2_AttentionUNet3D', patient_idx, 'pred'),\n",
    "    'AttentionUNet3D_DenseASPP': get_nii_path(output_dir, '2_AttentionUNet3D_DenseASPP', patient_idx, 'pred'),\n",
    "    'AttentionUNet3D_SCGA': get_nii_path(output_dir, '2_AttentionUNet3D_SCGA', patient_idx, 'pred'),\n",
    "    'AA_UNet': get_nii_path(output_dir, '3_AA_UNet', patient_idx, 'pred')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nii(nii_path, type='mask', trans_position=(0, 1, 2)):\n",
    "    \n",
    "    if type == 'mask':\n",
    "        return nib.load(nii_path).get_fdata().transpose(trans_position).astype(np.uint8)\n",
    "    elif type =='image':\n",
    "        return nib.load(nii_path).get_fdata().transpose(trans_position)\n",
    "    else:\n",
    "        raise ValueError('type must be mask or image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_data = load_nii(nii_path, type='image', trans_position=(2, 0, 1))\n",
    "gt_data = load_nii(gt_path, type='mask', trans_position=(2, 0, 1))\n",
    "\n",
    "pred_dict = {}\n",
    "for key, path in pred_paths.items():\n",
    "    pred_data = load_nii(path, type='mask', trans_position=(2, 0, 1))\n",
    "    pred_dict[key] = pred_data\n",
    "    print(f'{key} pred shape: {pred_data.shape}')\n",
    "\n",
    "print(nii_data.shape)\n",
    "print(gt_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(nii_data))\n",
    "print(np.unique(gt_data))\n",
    "print(np.unique(pred_dict['UNet3D']))\n",
    "print(np.unique(pred_dict['AttentionUNet3D']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import numpy as np\n",
    "\n",
    "# 创建离散Colormap（含归一化）\n",
    "color_list = [(0,0,0), (1,0,0), (0,1,0), (0,0,1)]\n",
    "cmap = ListedColormap(color_list, name='custom_discrete', N=4)\n",
    "bounds = [0, 1, 2, 3, 4]  # 类别边界\n",
    "norm = BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "def get_slices(nii_data, gt_data, pred_dict, slice_idx=80, axis=0):\n",
    "    pred_slices = {}\n",
    "    if axis == 0:\n",
    "        nii_slice = nii_data[slice_idx]\n",
    "        gt_slice = gt_data[slice_idx]\n",
    "        for k, v in pred_dict.items():\n",
    "            pred_slices[k] = v[slice_idx]\n",
    "    elif axis == 1:\n",
    "        nii_slice = nii_data[:, slice_idx]\n",
    "        gt_slice = gt_data[:, slice_idx]\n",
    "        for k, v in pred_dict.items():\n",
    "            pred_slices[k] = v[:, slice_idx]\n",
    "    elif axis == 2:\n",
    "        nii_slice = nii_data[:, :, slice_idx]\n",
    "        gt_slice = gt_data[:, :, slice_idx]\n",
    "        \n",
    "    return nii_slice, gt_slice, pred_slices\n",
    "\n",
    "def plot_slice(nii_slice, gt_slice, pred_slices, title: str, overlay_cmap: str = cmap, alpha: float = 1, axis=0):\n",
    "    \n",
    "    if axis == 0:\n",
    "        figsize = (18, 10)\n",
    "    elif axis == 1 or axis == 2:\n",
    "        figsize = (10, 18)\n",
    "        \n",
    "    fig, axes = plt.subplots(1, len(pred_slices)+1, figsize=figsize)\n",
    "    \n",
    "    # 显示原始图像和GT\n",
    "    axes[0].imshow(nii_slice, cmap='gray')  # 底层图像用灰度\n",
    "    axes[0].imshow(np.ma.masked_where(gt_slice == 0, gt_slice), \n",
    "                   cmap=overlay_cmap, norm=norm, alpha=alpha)  # GT用自定义Colormap\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    axes[0].axis('off')  # 关闭坐标轴\n",
    "    # 显示预测结果\n",
    "    for i, (model_name, overlay) in enumerate(pred_slices.items()):\n",
    "        axes[i+1].imshow(nii_slice, cmap='gray')        # 底层图像用灰度\n",
    "        axes[i+1].imshow(np.ma.masked_where(overlay == 0, overlay),  # 预测结果用自定义Colormap\n",
    "                         cmap=overlay_cmap, norm=norm, alpha=alpha)\n",
    "        axes[i+1].set_title(f\"{model_name} Prediction\") \n",
    "        axes[i+1].axis('off')  # 关闭坐标轴\n",
    "    fig.tight_layout()\n",
    "    axes[i+1].axis('off')  # 关闭坐标轴\n",
    "    fig.show() \n",
    "\n",
    "\n",
    "def plot_volume(nii_data, gt_data, pred_dict, title: str, overlay_cmap: str = cmap, alpha: float = 1, axis=0):\n",
    "\n",
    "    for slice in range(nii_data.shape[axis]):\n",
    "        nii_slice, gt_slice, pred_slices = get_slices(nii_data, gt_data, pred_dict, slice_idx=slice, axis=axis)\n",
    "        plot_slice(nii_slice, gt_slice, pred_slices, title=f'Slice {slice} {title}', overlay_cmap=overlay_cmap, alpha=alpha, axis=axis)\n",
    "        \n",
    "\n",
    "slice_idx_axis0 = 50\n",
    "slice_idx_axis1 = 170\n",
    "slice_idx_axis2 = 100\n",
    "\n",
    "nii_slice, gt_slice, pred_slices = get_slices(nii_data, gt_data, pred_dict, slice_idx=slice_idx_axis0, axis=0)\n",
    "plot_slice(nii_slice, gt_slice, pred_slices, title=f'Slice {slice_idx_axis0} Axis 横断面', axis=0)\n",
    "\n",
    "nii_slice, gt_slice, pred_slices = get_slices(nii_data, gt_data, pred_dict, slice_idx=slice_idx_axis1, axis=1)\n",
    "plot_slice(nii_slice, gt_slice, pred_slices, title=f'Slice {slice_idx_axis1} Axis 矢状面', axis=1)\n",
    "\n",
    "# nii_slice, gt_slice, pred_slices = get_slices(nii_data, gt_data, pred_dict, slice_idx=slice_idx_axis2, axis=2)\n",
    "# plot_slice(nii_slice, gt_slice, pred_slices, title=f'Slice {slice_idx_axis2} Axis 冠状面', axis=1)\n",
    "plot_volume(nii_data, gt_data, pred_dict, title='Volume')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import numpy as np\n",
    "\n",
    "# 创建离散Colormap（含归一化）\n",
    "color_list = [(0,0,0), (1,0,0), (0,1,0), (0,0,1)]\n",
    "cmap = ListedColormap(color_list, name='custom_discrete', N=4)\n",
    "bounds = [0, 1, 2, 3, 4]  # 类别边界\n",
    "norm = BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "def plot_slice(nii_slice, gt_slice, pred_slices, title: str, overlay_cmap: str = 'cool', alpha: float = 1):\n",
    "    fig, axes = plt.subplots(1, len(pred_slices)+1, figsize=(18, 10))\n",
    "    \n",
    "    # 显示原始图像和GT\n",
    "    axes[0].imshow(nii_slice, cmap='gray')  # 底层图像用灰度\n",
    "    axes[0].imshow(np.ma.masked_where(gt_slice == 0, gt_slice), \n",
    "                   cmap=overlay_cmap, norm=norm, alpha=alpha)  # GT用自定义Colormap\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    \n",
    "    # 显示预测结果\n",
    "    for i, (model_name, overlay) in enumerate(pred_slices.items()):\n",
    "        axes[i+1].imshow(nii_slice, cmap='gray')        # 底层图像用灰度\n",
    "        axes[i+1].imshow(np.ma.masked_where(overlay == 0, overlay),  # 预测结果用自定义Colormap\n",
    "                         cmap=overlay_cmap, norm=norm, alpha=alpha)\n",
    "        axes[i+1].set_title(f\"{model_name} Prediction\") \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "nii_slice, gt_slice, pred_slices = get_slices(nii_data, gt_data, pred_dict, slice_idx=50)\n",
    "plot_slice(nii_slice, gt_slice, pred_slices, title='Slice 100', overlay_cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_slice_slider(nii_data, initial_slice=0):\n",
    "    \"\"\"创建切片滑动组件\"\"\"\n",
    "    # 创建交互控件\n",
    "    slider = widgets.IntSlider(\n",
    "        value=initial_slice,\n",
    "        min=0,\n",
    "        max=nii_data.shape[0]-1,  # 根据数据维度动态设置最大值\n",
    "        step=1,\n",
    "        description='Slice Index:',\n",
    "        continuous_update=False  # 仅在释放滑块时更新（性能优化）\n",
    "    )\n",
    "    current_label = widgets.Label(value=f\"当前切片：{initial_slice}\")\n",
    "    \n",
    "    # 控件布局美化\n",
    "    slider.layout.width = '600px'\n",
    "    controls = widgets.VBox([slider, current_label])\n",
    "    \n",
    "    # 初始化图像显示\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    img = ax.imshow(nii_data[initial_slice], cmap='gray')\n",
    "    plt.close(fig)  # 避免重复显示初始图像\n",
    "    \n",
    "    # 定义更新回调函数\n",
    "    def update_slice(change):\n",
    "        img.set_data(nii_data[change['new']])\n",
    "        current_label.value = f\"当前切片：{change['new']}\"\n",
    "        fig.canvas.draw_idle()\n",
    "    \n",
    "    # 绑定事件\n",
    "    slider.observe(update_slice, names='value')\n",
    "    \n",
    "    # 组合显示\n",
    "    return widgets.VBox([controls, fig.canvas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
